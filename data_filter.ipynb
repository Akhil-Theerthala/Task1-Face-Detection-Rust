{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd26ba61",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Based on the WiderFace dataset, this code contains a sample of ~12k images with one or more faces. The dataset has been transformed to a format suitable for training a face detection model using the `ultralytics` library.\n",
    "\n",
    "* Since the images can contain more than one face, and the fact that we need suitable resolution of images for face-generation modules, the images have been filtered to only those with reasonable number of faces, with reasonable area of the face in the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a7a97b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Size: 12880\n",
      "Filtered Dataset Size: 8439\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "bounding_boxes = os.listdir(\"Data/labels\")\n",
    "\n",
    "print(\"Original Dataset Size:\", len(bounding_boxes))\n",
    "\n",
    "def contains_too_many_faces(label_file, max_faces=5):\n",
    "    \"\"\"\n",
    "    Check if the bounding boxes contain too many faces.\n",
    "    \"\"\"\n",
    "    \n",
    "    #get number of lines in the label file\n",
    "    with open(os.path.join(\"Data/labels\", label_file), 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    return len(lines) > max_faces\n",
    "\n",
    "bounding_boxes = [bb for bb in bounding_boxes if not contains_too_many_faces(bb, max_faces=5)]\n",
    "print(\"Filtered Dataset Size:\", len(bounding_boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43396188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15th quantile: 0.33%\n",
      "25th quantile: 0.69%\n",
      "30th quantile: 0.89%\n",
      "35th quantile: 1.14%\n",
      "50th quantile: 2.12%\n",
      "75th quantile: 5.63%\n",
      "90th quantile: 11.88%\n",
      "95th quantile: 17.07%\n",
      "99th quantile: 31.87%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_area_of_faces(label_file):\n",
    "    \"\"\"\n",
    "    Using the normalized bounding boxes, calculate the least area of the face\n",
    "    \"\"\"\n",
    "    with open(os.path.join(\"Data/labels\", label_file), 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        bboxes = [list(map(float, line.strip().split()[1:])) for line in lines]\n",
    "    \n",
    "    areas = [np.abs((bbox[2] - bbox[0]) * (bbox[3] - bbox[1])) for bbox in bboxes]\n",
    "    return min(areas) if areas else 0\n",
    "\n",
    "bbox_areas = {file_name: get_area_of_faces(file_name) for file_name in bounding_boxes}\n",
    "\n",
    "areas = np.array(list(bbox_areas.values()))\n",
    "#print quantiles of the areas\n",
    "quantiles = [0.15, 0.25, 0.3, 0.35, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "for quantile in quantiles:\n",
    "    print(f\"{int(quantile*100)}th quantile: {np.quantile(areas, quantile)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b38758",
   "metadata": {},
   "source": [
    "* Since the original images are scaled down to 640x640, then the detections of 1% are going to be 64x64. Anything smaller than that would be discarded. We would need to filter the images by ~4% threshold if we want the faces to be at least 128x128 pixels. However, since this is only a detection model, the size of the face is not critical. Hence only the noisy samples are being removed. Simiarly, the outlying 1% of the images with more than 30% of the image area occupied by faces are also removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05cad0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dataset Size: 5627\n"
     ]
    }
   ],
   "source": [
    "bbox_areas = {file_name: area for file_name, area in bbox_areas.items() if (area > 0.01) & (area <0.3187)}\n",
    "bounding_boxes = list(bbox_areas.keys())\n",
    "print(\"Final Dataset Size:\", len(bounding_boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84282dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "label_path =\"Data/labels\"\n",
    "image_path = \"Data/images\"\n",
    "\n",
    "filtered_labels = \"Data/filtered_labels\"\n",
    "filtered_images = \"Data/filtered_images\"\n",
    "os.makedirs(filtered_labels, exist_ok=True)\n",
    "os.makedirs(filtered_images, exist_ok=True)\n",
    "\n",
    "for filtered_file in bounding_boxes:\n",
    "    #copy the label file\n",
    "    shutil.copy(os.path.join(label_path, filtered_file), os.path.join(filtered_labels, filtered_file))\n",
    "    \n",
    "    #copy the image file\n",
    "    image_file = filtered_file.replace(\".txt\", \".jpg\")\n",
    "    shutil.copy(os.path.join(image_path, image_file), os.path.join(filtered_images, image_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
