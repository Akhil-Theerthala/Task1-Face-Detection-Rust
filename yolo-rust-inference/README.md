# YOLO Rust Inference

This project implements an inference pipeline for the YOLOv11 model using Rust. The pipeline takes a directory of images as input and outputs predictions for each image in separate text files.

## Project Structure

```
yolo-rust-inference
├── src
│   ├── main.rs          # Entry point of the application
│   ├── inference.rs     # Contains the InferencePipeline struct
│   ├── utils.rs         # Utility functions for file handling
│   └── models
│       └── mod.rs       # Model-related structures and types
├── models
│   └── yolov11.onnx     # YOLOv11 ONNX model file
├── Cargo.toml           # Rust project configuration file
└── README.md            # Project documentation
```

## Setup Instructions

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd yolo-rust-inference
   ```

2. **Install Rust:**
   Follow the instructions at [rust-lang.org](https://www.rust-lang.org/tools/install) to install Rust and Cargo.

3. **Add dependencies:**
   Update the `Cargo.toml` file with necessary dependencies for ONNX runtime and image processing libraries.

4. **Download the YOLOv11 model:**
   Place the `yolov11.onnx` model file in the `models` directory.

## Usage

To run the inference pipeline, use the following command:

```bash
cargo run -- <image_directory>
```

Replace `<image_directory>` with the path to the directory containing the images you want to process.

## Output

The predictions for each image will be saved in text files named `<image_name>.txt` in the same directory as the images.

## Notes

- Ensure that the images in the specified directory are in a supported format (e.g., JPEG, PNG).
- The output text files will contain the predictions generated by the YOLOv11 model for each corresponding image.